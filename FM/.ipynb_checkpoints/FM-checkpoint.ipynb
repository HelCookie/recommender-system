{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集\n",
    "MovieLens 100k\n",
    "\n",
    "训练集\n",
    "\n",
    "测试集\n",
    "\n",
    "userid | itemid | rating | timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FM用于CTR预估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ音乐数据\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['user','item','rating','timestamp']\n",
    "# train_df = pd.read_csv('../data/ml-100k/ua.base',delimiter = '\\t',names = cols)\n",
    "# test_df = pd.read_csv('../data/ml-100k/ua.test',delimiter='\\t', names= cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zjw/anaconda3/envs/tf/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "cols = [str(i) for i in range(10)] # 第一列label，其余列为feature\n",
    "all_df = pd.read_csv('data/test.txt',header=0,delimiter = ',',names = cols)\n",
    "\n",
    "\n",
    "feature_df = all_df[cols[1:]]\n",
    "minmax_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "scaled_feature = minmax_scaler.fit_transform(feature_df)\n",
    "x_train,x_test,y_train,y_test = train_test_split(scaled_feature, all_df[cols[0]], train_size = 0.8, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = feature_df.shape[1]\n",
    "embedding_size = 5\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, feature_size] , name='features')\n",
    "y = tf.placeholder(tf.float32, [None] , name='labels')\n",
    "\n",
    "embeddings = tf.Variable(tf.random_normal([feature_size, embedding_size], mean=0.0, stddev=0.01), \n",
    "                         name='feature_embeddings') # \n",
    "feature_value = tf.reshape(tf.cast(x,tf.float32), shape=[-1, feature_size, 1])\n",
    "w0 = tf.Variable(tf.random_normal([1], mean=0.0, stddev=0.01))\n",
    "\n",
    "# ------ first-order term ------\n",
    "feature_bias = tf.Variable(tf.random_uniform([feature_size, 1], 0.0, 1.0), name='feature_bias') # 初始化w_i\n",
    "y_first_order = feature_bias\n",
    "y_first_order = tf.reduce_sum(tf.multiply(y_first_order, feature_value), axis=2) # w_i * x_i  (n * k)\n",
    "\n",
    "# ------ second-order term ------\n",
    "# 和平方部分\n",
    "embeddings = tf.multiply(embeddings, feature_value) # v_i * x_i\n",
    "summed_feature_emb = tf.reduce_sum(embeddings,axis=1)     # n * k\n",
    "summed_feature_emb_square = tf.square(summed_feature_emb) # n * k \n",
    "\n",
    "# 平方和部分\n",
    "squared_feature_emb = tf.square(embeddings)\n",
    "squared_sum_feature_emb = tf.reduce_sum(squared_feature_emb, axis = 1)\n",
    "\n",
    "# 和平方部分与平方和部分组合\n",
    "y_second_order = 0.5 * tf.subtract(summed_feature_emb_square, squared_sum_feature_emb)\n",
    "\n",
    "y_predict = tf.add(tf.reduce_sum(y_first_order,1), tf.reduce_sum(y_second_order,1))\n",
    "y_predict = tf.add(w0, y_predict)\n",
    "# y_predict = tf.nn.sigmoid(y_predict)\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "# loss定义为均方差\n",
    "loss = tf.reduce_mean(tf.square(y_predict - y))\n",
    "auc = tf.contrib.metrics.streaming_auc(y_predict,tf.convert_to_tensor(y))  \n",
    "train = tf.train.AdamOptimizer(learning_rate=0.01,epsilon=1e-8).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "[0]loss:0.185139\n",
      "[10]loss:0.179217\n",
      "[20]loss:0.176569\n",
      "[30]loss:0.172604\n",
      "[40]loss:0.169132\n",
      "[50]loss:0.166963\n",
      "[60]loss:0.165328\n",
      "[70]loss:0.163931\n",
      "[80]loss:0.162557\n",
      "[90]loss:0.161241\n",
      "[100]loss:0.160135\n",
      "[110]loss:0.159292\n",
      "[120]loss:0.158681\n",
      "[130]loss:0.158234\n",
      "[140]loss:0.157884\n",
      "[150]loss:0.157595\n",
      "[160]loss:0.157353\n",
      "[170]loss:0.157149\n",
      "[180]loss:0.156979\n",
      "[190]loss:0.156837\n",
      "testing...\n",
      "MSE: 0.156734\n",
      "AUC: (0.0, 0.74744475)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    print('training...')\n",
    "    for _ in range(epochs):\n",
    "        sess.run(train, feed_dict = {x:x_train, y: y_train})\n",
    "        if _ % 10 == 0:\n",
    "            print(\"[%d]loss:%s\"%(_,sess.run(loss, feed_dict={x: x_train, y: y_train})))\n",
    "    \n",
    "    print('testing...')\n",
    "    print('MSE:',sess.run(loss, feed_dict={x: x_test ,y: y_test}))\n",
    "    print('AUC:',sess.run(auc, feed_dict={x: x_test ,y: y_test}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
